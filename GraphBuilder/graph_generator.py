import os
import subprocess
import logging
from typing import Optional, Tuple, List
from Utils.utils import *
import pickle
import sqlite3
import multiprocessing
from DataProcess.dataset_utils import FileFilter, find_llvm_files, cleanup_residual_temp_files
import typer

from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn, TimeRemainingColumn

console = Console()

# Global logger for multiprocessing
logger = logging.getLogger(__name__)

def _opt_initial_purify(llvm_ir_path: str) -> Optional[str]:
    """Strip all metadata from LLVM IR using custom pass"""
    purified_file = os.path.splitext(llvm_ir_path)[0] + '_purified.ll'
    new_path = os.path.dirname(llvm_ir_path)
    
    try:
        cmd = [
            "opt",
            "-S",
            "-load-pass-plugin=" + DEFAULT_PURIFY_SO_PATH,
            "--passes=strip-all-metadata",
            "-non-global-value-max-name-size=16384",
            os.path.basename(llvm_ir_path),
            "-o", os.path.basename(purified_file)
        ]

        p = subprocess.run(cmd, text=True, capture_output=True, cwd=new_path)
        stdout, stderr = p.stdout, p.stderr
        if p.returncode != 0:
            error_message = f"opt failed with return code {p.returncode}, stdout: {stdout}, stderr: {stderr}"
            logger.error(error_message)
            raise ValueError(error_message)
            
        if not os.path.exists(os.path.join(new_path, os.path.basename(purified_file))):
            error_message = f"Purified file was not created: {purified_file}"
            logger.error(error_message)
            raise ValueError(error_message)
            
        return os.path.join(new_path, os.path.basename(purified_file))
        
    except Exception as e:
        logger.error(f"Exception in metadata stripping for {llvm_ir_path}: {e}")
        return None
        
def _opt_generate_ddg(purified_llvm_ir_path: str) -> Optional[Tuple[str, str]]:
    """Generate DDG using opt tool on purified IR"""
    instrumented_file = os.path.splitext(purified_llvm_ir_path)[0] + '_instrumented.ll'
    new_path = os.path.dirname(purified_llvm_ir_path)
    
    try:
        cmd = [
            "opt",
            "-load-pass-plugin=" + DEFAULT_DDG_SO_PATH,
            "-passes=dot-id-graph",
            "-non-global-value-max-name-size=16384",
            os.path.basename(purified_llvm_ir_path),
            "-S",
            "-o", os.path.basename(instrumented_file)
        ]

        p = subprocess.run(cmd, text=True, capture_output=True, cwd=new_path)
        stdout, stderr = p.stdout, p.stderr

        dot_file = None
        for line in stderr.splitlines():
            if "Writing ID-tagged graph to" in line:
                dot_file = line.split("'")[1]
                break
        else:
            logger.error("No dot file generated by opt, output is" f"{stdout} | stderr: {stderr}")
            return None
                
        if dot_file is None:
            return None
            
        dot_file = os.path.join(new_path, dot_file)
        return instrumented_file, dot_file
        
    except Exception as e:
        logger.error(f"Exception in DDG generation for {purified_llvm_ir_path}: {e}")
        return None
        
def _opt_generate_cfg(purified_llvm_ir_path: str) -> Optional[str]:
    """Generate CFG using opt tool on purified IR"""
    new_path = os.path.dirname(purified_llvm_ir_path)
    
    try:
        cmd = [
            "opt",
            "-load-pass-plugin=" + DEFAULT_CFG_SO_PATH,
            "-passes=dot-my-cfg",
            "-non-global-value-max-name-size=16384",
            os.path.basename(purified_llvm_ir_path),
            "-o", "/dev/null"
        ]

        p = subprocess.run(cmd, text=True, capture_output=True, cwd=new_path)
        stdout, stderr = p.stdout, p.stderr

        dot_file = None
        for line in stderr.splitlines():
            if "Write CFG to " in line:
                dot_file = line.split("'")[1]
                break
        else:
            logger.error("No dot file generated by opt, output is" f"{stdout} | stderr: {stderr}")
            return None
        
        if dot_file is None:
            return None
            
        return os.path.join(new_path, dot_file)
        
    except Exception as e:
        logger.error(f"Exception in CFG generation for {purified_llvm_ir_path}: {e}")
        return None
    
def process_one_file(llvm_ir_path: str) -> Optional[Tuple[str, str, str, str]]:
    """
    Process a single LLVM IR file to generate purified IR, DDG, and CFG.
    
    Args:
        llvm_ir_path: Path to the LLVM IR file.
        
    Returns:
        Tuple of paths (purified_llvm_ir_path, ddg_dot_file, cfg_dot_file) or None if any step fails.
    """
    purified_llvm_ir_path = _opt_initial_purify(llvm_ir_path)
    if purified_llvm_ir_path is None:
        return None
    
    ddg_result = _opt_generate_ddg(purified_llvm_ir_path)
    if ddg_result is None:
        return None
    
    instrumented_file, ddg_dot_file = ddg_result
    
    cfg_dot_file = _opt_generate_cfg(purified_llvm_ir_path)
    if cfg_dot_file is None:
        return None
    
    return llvm_ir_path, purified_llvm_ir_path, instrumented_file, cfg_dot_file, ddg_dot_file


def process_chunk(file_paths: List[str], queue: multiprocessing.Queue) -> List[Optional[Tuple[str, str, str, str]]]:
    results = []
    for file_path in file_paths:
        result = process_one_file(file_path)
        if result is not None:
            results.append(result)
    queue.put(results)  # Send results back to the main process


def init_database(output_db_path: str):
    """Initialize the SQLite database for storing results."""
    with sqlite3.connect(output_db_path,) as conn:
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS results (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            input_path TEXT UNIQUE,
            purify_path TEXT,
            instrumented_path TEXT,
            cfg_dot TEXT,
            ddg_dot TEXT
            )
        """)
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_input_path ON results(input_path)")
        conn.commit()


def file_writer(queue: multiprocessing.Queue, output_db_path: str):
    with sqlite3.connect(output_db_path) as conn:
        cursor = conn.cursor()
        
        while True:
            results = queue.get()
            if isinstance(results, str) and results == "STOP":
                break
            
            # Use executemany for batch insert - much more efficient
            cursor.executemany("""
                INSERT OR REPLACE INTO results (input_path, purify_path, instrumented_path, cfg_dot, ddg_dot)
                VALUES (?, ?, ?, ?, ?)
            """, results)
            
            # Commit every batch_size batches instead of every batch
            conn.commit()
        
        # Final commit for any remaining data
        conn.commit()


def pipeline(input_dir: str, cleanup: bool = True, use_cache: bool = True, jobs: int = multiprocessing.cpu_count()):
    if cleanup:
        cleanup_residual_temp_files(input_dir)


    cache_path = os.path.join(input_dir, 'filecache.pkl')
    db_path = os.path.join(input_dir, 'results.db')
    if use_cache and os.path.exists(cache_path):
        with open(cache_path, 'rb') as f:
            valid_files = pickle.load(f)
            console.print(f"[green]Loaded {len(valid_files)} valid files from cache.[/green]")

    else:
        llvm_files = find_llvm_files(input_dir, pattern="**/*_functions/*.ll")
        llvm_files = [os.path.abspath(f) for f in llvm_files]
        
        # Remove those files end with '.dot' or '_instrumented.ll' or '_purified.ll'
        llvm_files = [f for f in llvm_files if not (f.endswith('.dot') or 
                                                    f.endswith('_instrumented.ll') or 
                                                    f.endswith('_purified.ll'))]
        file_filter = FileFilter(num_processes=multiprocessing.cpu_count())
        valid_files = file_filter.filter_files_parallel(llvm_files)
        if use_cache:
            with open(cache_path, 'wb') as f:
                pickle.dump(valid_files, f)
            console.print(f"[green]Saved {len(valid_files)} valid files to cache.[/green]")

    console.print(f"[yellow]Filtering finished Files...[/yellow]")
    init_database(db_path)

    # Get all input_path from database and filter out already processed files
    with sqlite3.connect(db_path) as conn:
        cursor = conn.cursor()
        cursor.execute("SELECT input_path FROM results")
        processed_files: set = {row[0] for row in cursor.fetchall()}
    
    valid_files = [f for f in valid_files if f not in processed_files]
    console.print(f"[green]Filtered {len(valid_files)} files to process.[/green]")
    queue = multiprocessing.Manager().Queue()
    writer_process = multiprocessing.Process(target=file_writer, args=(queue, db_path))
    writer_process.start()

    # Use multiprocessing to process files in parallel
    console.print("[cyan]Starting file processing in parallel...[/cyan]")

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TextColumn("[cyan]{task.completed}/{task.total}"),
        TimeElapsedColumn(),
        TimeRemainingColumn()
    ) as progress:

        pool = multiprocessing.Pool(processes=jobs)

        task = progress.add_task("[cyan]Processing files...", total=len(valid_files))
        chunk_size = 100
        for i in range(0, len(valid_files), chunk_size):
            chunk = valid_files[i:i + chunk_size]
            current_chunk_size = len(chunk)
            pool.apply_async(process_chunk, args=(chunk, queue), callback=lambda _, size=current_chunk_size: progress.update(task, advance=size))

        pool.close()
        pool.join()
    
        queue.put("STOP")  # Signal the writer process to stop
        writer_process.join()
    
    console.print("[green]Pipeline completed successfully![/green]")
    console.print(f"[blue]Results saved to {db_path}[/blue]")

app = typer.Typer()

@app.command()
def main(
    input_dir: str = typer.Argument(..., help="Input directory containing LLVM IR files"),
    cleanup: bool = typer.Option(True, "--cleanup/--no-cleanup", help="Clean up residual temp files before processing"),
    use_cache: bool = typer.Option(True, "--cache/--no-cache", help="Use cached file list if available"),
    jobs: int = typer.Option(multiprocessing.cpu_count(), "--jobs", "-j", help="Number of parallel jobs to run (default: number of CPU cores)")
):
    """
    Process LLVM IR files to generate purified IR, DDG, and CFG files.
    
    This tool processes LLVM IR files in parallel, generating:
    - Purified LLVM IR (metadata stripped)
    - Data Dependence Graph (DDG) in DOT format
    - Control Flow Graph (CFG) in DOT format
    
    Results are stored in a SQLite database for efficient querying.
    """
    pipeline(input_dir, cleanup, use_cache, jobs)

if __name__ == "__main__":
    app()